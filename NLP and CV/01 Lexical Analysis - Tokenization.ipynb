{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e6ec40",
   "metadata": {},
   "source": [
    "## Installations\n",
    "-pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8950cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f26572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (3.7.4)\n",
      "Requirement already satisfied: textblob in c:\\programdata\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy textblob -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec7b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  #tokenization\n",
    "nltk.download('stopwords') #stopwiords removal\n",
    "nltk.download('averaged_perceptron_tagger') #POS tagging\n",
    "nltk.download('wordnet') #wordnet database and lemmatization\n",
    "nltk.download('omw-1.4') #Stemming\n",
    "nltk.download('indian') #Indain language POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d0b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker') #chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46e23fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb1bb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='They told that their ages are 25 27 and 31 respectively '\n",
    "\n",
    "## find the average of ages mentioned in the above sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3b2c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "471d3ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'told',\n",
       " 'that',\n",
       " 'their',\n",
       " 'ages',\n",
       " 'are',\n",
       " '25',\n",
       " '27',\n",
       " 'and',\n",
       " '31',\n",
       " 'respectively']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09041cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum=0\n",
    "cnt=0\n",
    "for i in s1:\n",
    "    if i.isnumeric():\n",
    "        cnt+=1\n",
    "        sum+=int(i)\n",
    "avg=sum/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba87894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.666666666666668\n"
     ]
    }
   ],
   "source": [
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384cf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85bd082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "for word in sent.split():\n",
    "    if word.isdigit():\n",
    "        ages.append(int(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7205888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 27, 31]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12e7dfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ages) / len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fedc203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent='They told that their ages are 25 27 and 31 respectively '\n",
    "\n",
    "ages = [int(word) for word in sent.split() if word.isdigit()]\n",
    "np.sum(ages) / len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeb36a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437422b",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8212e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8b2c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the functions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edb925d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#segmenation:convert entire sentence into chunks of sentences\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12f72347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert entire sentence into separate words\n",
    "wrd=word_tokenize(sent)\n",
    "wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3c3d421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "puc=['!','?','.']\n",
    "for i in wrd:\n",
    "    if i in puc:\n",
    "        cnt+=1\n",
    "percentage=(cnt/len(wrd))*100\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f66a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in wrd:\n",
    "    if not i.isalnum():\n",
    "        cnt+=1\n",
    "percentage=(cnt/len(wrd))*100\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18e2d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Percentage is :  0.25\n"
     ]
    }
   ],
   "source": [
    "text = len([word for word in word_tokenize(sent) if not word.isalnum()])\n",
    "print(text)\n",
    "print(\"Percentage is : \",text / len(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01e5a5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('y')   #asci value of any characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c102ea61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c34f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b28eeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('K')\n",
    "sys.getsizeof('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "345723f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'abcdef'\n",
    "sys.getsizeof(char) # 50 initial, afterward 1 char 1 byte -> 50+1+1+1+1+1 = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7f8fb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97) #ascii value to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56891105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à²…'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(3205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "341d3472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤µ\n"
     ]
    }
   ],
   "source": [
    "char='\\u0935'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dc009044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "à¤µà¥€\n"
     ]
    }
   ],
   "source": [
    "char='\\u0935\\u0940'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cf9092bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'é'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x9350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bec3cd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('é')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e1c2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "name='à¤…à¤®à¥‡à¤¯ à¤•à¥‡ à¤ªà¥à¤°à¤£à¤µ'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a0b4663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤…à¤®à¥‡à¤¯', 'à¤•à¥‡', 'à¤ªà¥à¤°à¤£à¤µ']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ca7c9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_text='à¤¯à¤¹ à¤ªà¥à¤£à¥‡ à¤•à¥‡ à¤¦à¤•à¥à¤·à¤¿à¤£-à¤ªà¤¶à¥à¤šà¤¿à¤® à¤®à¥‡à¤‚ à¤²à¤—à¤­à¤— 60 à¤•à¤¿à¤®à¥€ (37 à¤®à¥€à¤²) à¤”à¤° à¤¸à¤¹à¥à¤¯à¤¾à¤¦à¥à¤°à¤¿ à¤®à¥‡à¤‚ à¤¨à¤¾à¤¸à¤°à¤ªà¥à¤° à¤¸à¥‡ à¤²à¤—à¤­à¤— 15 à¤•à¤¿.à¤®à¥€. (9.3 à¤®à¥€à¤²) à¤¦à¥‚à¤° à¤¸à¥à¤¥à¤¿à¤¤ à¤¹à¥ˆà¥¤ à¤¯à¤¹ à¤•à¤¿à¤²à¤¾ à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤² à¤¸à¥‡ 1,376 à¤®à¥€à¤Ÿà¤° (4,514 à¤«à¥€à¤Ÿ) à¤Šà¤à¤šà¤¾ à¤¹à¥ˆà¥¤ à¤•à¤¿à¤²à¥‡ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤•à¤¾ à¤µà¥à¤¯à¤¾à¤¸ à¤²à¤—à¤­à¤— 40 à¤•à¤¿à¤®à¥€ (25 à¤®à¥€à¤²) à¤¹à¥ˆ, à¤œà¤¿à¤¸à¤¨à¥‡ à¤‡à¤¸ à¤ªà¤° à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸ à¤®à¥‡à¤‚ à¤˜à¥‡à¤°à¤¾à¤¬à¤‚à¤¦à¥€ à¤•à¤°à¤¨à¤¾ à¤®à¥à¤¶à¥à¤•à¤¿à¤² à¤¬à¤¨à¤¾ à¤¦à¤¿à¤¯à¤¾ à¤¥à¤¾à¥¤ à¤•à¤¿à¤²à¥‡ à¤•à¥‡ à¤–à¤‚à¤¡à¤¹à¤°à¥‹à¤‚ à¤®à¥‡à¤‚ à¤ªà¤¾à¤¨à¥€ à¤•à¥‡ à¤•à¥à¤‚à¤¡ à¤”à¤° à¤—à¥à¤«à¤¾à¤à¤ à¤ªà¤¾à¤à¤ à¤—à¤ à¤¹à¥ˆà¤‚à¥¤ à¤¯à¤¹ à¤•à¤¿à¤²à¤¾ à¤®à¥à¤°à¥à¤®à¤¾à¤¦à¥‡à¤µà¥€ à¤¡à¥‹à¤‚à¤—à¤° (à¤¦à¥‡à¤µà¥€ à¤®à¥à¤°à¥à¤®à¥à¤¬à¤¾ à¤•à¤¾ à¤ªà¤°à¥à¤µà¤¤) à¤¨à¤¾à¤®à¤• à¤ªà¤¹à¤¾à¤¡à¤¼à¥€ à¤ªà¤° à¤¬à¤¨à¤¾à¤¯à¤¾ à¤—à¤¯à¤¾ à¤¥à¤¾à¥¤ à¤®à¤¾à¤¨à¤¾ à¤œà¤¾à¤¤à¤¾ à¤¹à¥ˆ à¤•à¤¿ à¤‡à¤¸à¤•à¥€ à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤ à¤­à¥Œà¤—à¥‹à¤²à¤¿à¤• à¤¸à¥à¤¥à¤¿à¤¤à¤¿ à¤•à¥‡ à¤•à¤¾à¤°à¤£ à¤¹à¥€ à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€ à¤¶à¤¿à¤µà¤¾à¤œà¥€ à¤®à¤¹à¤¾à¤°à¤¾à¤œ à¤¨à¥‡ à¤‡à¤¸à¥‡ à¤•à¤ˆ à¤¸à¤¾à¤² à¤¶à¤¾à¤¸à¤¨ à¤•à¤¾ à¤•à¥‡à¤¨à¥à¤¦à¥à¤° à¤¬à¤¨à¤¾à¤à¤‚ à¤°à¤–à¤¾à¥¤[2][3]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "71efe836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['à¤¯à¤¹',\n",
       " 'à¤ªà¥à¤£à¥‡',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤¦à¤•à¥à¤·à¤¿à¤£-à¤ªà¤¶à¥à¤šà¤¿à¤®',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤²à¤—à¤­à¤—',\n",
       " '60',\n",
       " 'à¤•à¤¿à¤®à¥€',\n",
       " '(',\n",
       " '37',\n",
       " 'à¤®à¥€à¤²',\n",
       " ')',\n",
       " 'à¤”à¤°',\n",
       " 'à¤¸à¤¹à¥à¤¯à¤¾à¤¦à¥à¤°à¤¿',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤¨à¤¾à¤¸à¤°à¤ªà¥à¤°',\n",
       " 'à¤¸à¥‡',\n",
       " 'à¤²à¤—à¤­à¤—',\n",
       " '15',\n",
       " 'à¤•à¤¿.à¤®à¥€',\n",
       " '.',\n",
       " '(',\n",
       " '9.3',\n",
       " 'à¤®à¥€à¤²',\n",
       " ')',\n",
       " 'à¤¦à¥‚à¤°',\n",
       " 'à¤¸à¥à¤¥à¤¿à¤¤',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤¯à¤¹',\n",
       " 'à¤•à¤¿à¤²à¤¾',\n",
       " 'à¤¸à¤®à¥à¤¦à¥à¤°',\n",
       " 'à¤¤à¤²',\n",
       " 'à¤¸à¥‡',\n",
       " '1,376',\n",
       " 'à¤®à¥€à¤Ÿà¤°',\n",
       " '(',\n",
       " '4,514',\n",
       " 'à¤«à¥€à¤Ÿ',\n",
       " ')',\n",
       " 'à¤Šà¤à¤šà¤¾',\n",
       " 'à¤¹à¥ˆà¥¤',\n",
       " 'à¤•à¤¿à¤²à¥‡',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤†à¤§à¤¾à¤°',\n",
       " 'à¤•à¤¾',\n",
       " 'à¤µà¥à¤¯à¤¾à¤¸',\n",
       " 'à¤²à¤—à¤­à¤—',\n",
       " '40',\n",
       " 'à¤•à¤¿à¤®à¥€',\n",
       " '(',\n",
       " '25',\n",
       " 'à¤®à¥€à¤²',\n",
       " ')',\n",
       " 'à¤¹à¥ˆ',\n",
       " ',',\n",
       " 'à¤œà¤¿à¤¸à¤¨à¥‡',\n",
       " 'à¤‡à¤¸',\n",
       " 'à¤ªà¤°',\n",
       " 'à¤‡à¤¤à¤¿à¤¹à¤¾à¤¸',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤˜à¥‡à¤°à¤¾à¤¬à¤‚à¤¦à¥€',\n",
       " 'à¤•à¤°à¤¨à¤¾',\n",
       " 'à¤®à¥à¤¶à¥à¤•à¤¿à¤²',\n",
       " 'à¤¬à¤¨à¤¾',\n",
       " 'à¤¦à¤¿à¤¯à¤¾',\n",
       " 'à¤¥à¤¾à¥¤',\n",
       " 'à¤•à¤¿à¤²à¥‡',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤–à¤‚à¤¡à¤¹à¤°à¥‹à¤‚',\n",
       " 'à¤®à¥‡à¤‚',\n",
       " 'à¤ªà¤¾à¤¨à¥€',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤•à¥à¤‚à¤¡',\n",
       " 'à¤”à¤°',\n",
       " 'à¤—à¥à¤«à¤¾à¤à¤',\n",
       " 'à¤ªà¤¾à¤à¤',\n",
       " 'à¤—à¤',\n",
       " 'à¤¹à¥ˆà¤‚à¥¤',\n",
       " 'à¤¯à¤¹',\n",
       " 'à¤•à¤¿à¤²à¤¾',\n",
       " 'à¤®à¥à¤°à¥à¤®à¤¾à¤¦à¥‡à¤µà¥€',\n",
       " 'à¤¡à¥‹à¤‚à¤—à¤°',\n",
       " '(',\n",
       " 'à¤¦à¥‡à¤µà¥€',\n",
       " 'à¤®à¥à¤°à¥à¤®à¥à¤¬à¤¾',\n",
       " 'à¤•à¤¾',\n",
       " 'à¤ªà¤°à¥à¤µà¤¤',\n",
       " ')',\n",
       " 'à¤¨à¤¾à¤®à¤•',\n",
       " 'à¤ªà¤¹à¤¾à¤¡à¤¼à¥€',\n",
       " 'à¤ªà¤°',\n",
       " 'à¤¬à¤¨à¤¾à¤¯à¤¾',\n",
       " 'à¤—à¤¯à¤¾',\n",
       " 'à¤¥à¤¾à¥¤',\n",
       " 'à¤®à¤¾à¤¨à¤¾',\n",
       " 'à¤œà¤¾à¤¤à¤¾',\n",
       " 'à¤¹à¥ˆ',\n",
       " 'à¤•à¤¿',\n",
       " 'à¤‡à¤¸à¤•à¥€',\n",
       " 'à¤¸à¥à¤°à¤•à¥à¤·à¤¿à¤¤',\n",
       " 'à¤­à¥Œà¤—à¥‹à¤²à¤¿à¤•',\n",
       " 'à¤¸à¥à¤¥à¤¿à¤¤à¤¿',\n",
       " 'à¤•à¥‡',\n",
       " 'à¤•à¤¾à¤°à¤£',\n",
       " 'à¤¹à¥€',\n",
       " 'à¤›à¤¤à¥à¤°à¤ªà¤¤à¥€',\n",
       " 'à¤¶à¤¿à¤µà¤¾à¤œà¥€',\n",
       " 'à¤®à¤¹à¤¾à¤°à¤¾à¤œ',\n",
       " 'à¤¨à¥‡',\n",
       " 'à¤‡à¤¸à¥‡',\n",
       " 'à¤•à¤ˆ',\n",
       " 'à¤¸à¤¾à¤²',\n",
       " 'à¤¶à¤¾à¤¸à¤¨',\n",
       " 'à¤•à¤¾',\n",
       " 'à¤•à¥‡à¤¨à¥à¤¦à¥à¤°',\n",
       " 'à¤¬à¤¨à¤¾à¤à¤‚',\n",
       " 'à¤°à¤–à¤¾à¥¤',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " '[',\n",
       " '3',\n",
       " ']']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(h_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d575f5",
   "metadata": {},
   "source": [
    "### Space tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18427193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! how are you?\n",
      "\n",
      "Welcome to the world of NLP.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "f1=open('mydata.txt','r')\n",
    "for ln in f1:\n",
    "    print(ln)\n",
    "f1.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f3e7d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "411e95f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " '\\thow',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " '\\tProgramming.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#space tokenizer\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "tk=SpaceTokenizer()\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d211cdf",
   "metadata": {},
   "source": [
    "## Tab Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "143b3436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9bdb0834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! ',\n",
       " 'how are you?\\nWelcome to the world of NLP ',\n",
       " 'Programming.\\n']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tab tokenizer\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "tk=TabTokenizer()\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f1dfd",
   "metadata": {},
   "source": [
    "## Line tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cffe9cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "761ef27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! \\thow are you?', 'Welcome to the world of NLP \\tProgramming.']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line tokenizer\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "lk=LineTokenizer()\n",
    "\n",
    "lk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01d6e5",
   "metadata": {},
   "source": [
    "### White space tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "495cca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e0aee4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#White tokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "wk=WhitespaceTokenizer()\n",
    "\n",
    "wk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5943d",
   "metadata": {},
   "source": [
    "### MWE tokenizer (Multi word expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ea66e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "10d757e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " 'o',\n",
       " ' ',\n",
       " 'F',\n",
       " 'r',\n",
       " 'i',\n",
       " 'e',\n",
       " 'n',\n",
       " 'd',\n",
       " 's',\n",
       " '!',\n",
       " ' ',\n",
       " '\\t',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " '?',\n",
       " '\\n',\n",
       " 'W',\n",
       " 'e',\n",
       " 'l',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'l',\n",
       " 'd',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'N',\n",
       " 'L',\n",
       " 'P',\n",
       " ' ',\n",
       " '\\t',\n",
       " 'P',\n",
       " 'r',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'm',\n",
       " 'm',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#White tokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "mwk=MWETokenizer()\n",
    "\n",
    "mwk.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c9c346fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Guido Van Rossum (Dutch: born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f9042a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guido',\n",
       " 'Van Rossum',\n",
       " '(',\n",
       " 'Dutch',\n",
       " ':',\n",
       " 'born',\n",
       " '31',\n",
       " 'January',\n",
       " '1956',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Dutch',\n",
       " 'programmer',\n",
       " 'best',\n",
       " 'known',\n",
       " 'as',\n",
       " 'the',\n",
       " 'creator',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'programming',\n",
       " 'language']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MWE tokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "mwk=MWETokenizer(separator=' ')  #separator given as space\n",
    "\n",
    "mwk.add_mwe(('Van', 'Rossum'))  #You want some words together always\n",
    "\n",
    "mwk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270ac4b",
   "metadata": {},
   "source": [
    "## Tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1e447b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Frie:)nds :)!  how are you?:( Welcome to the world of NLP  :D Programming.'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent =  'Hello Frie:)nds :)!  how are you?:( Welcome to the world of NLP  :D Programming.'\n",
    "\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9e1fd815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Frie',\n",
       " ':)',\n",
       " 'nds',\n",
       " ':)',\n",
       " '!',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " ':(',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " ':D',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweet tokenizer\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "\n",
    "twk=TweetTokenizer()\n",
    "\n",
    "twk.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "68410165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!  ðŸ˜Žhow are you?ðŸ‘¹?\n",
      "Welcome to the ðŸ¤© world of NLP ðŸ¤¯Programming.\n"
     ]
    }
   ],
   "source": [
    "f1=open('mydata.txt',encoding='utf-8')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9c97afb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'ðŸ˜Žhow',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ‘¹',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ðŸ¤©',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'ðŸ¤¯Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9d6fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!  ðŸ˜Žhow are you?ðŸ‘¹?\n",
      "Welcome to the ðŸ¤© world of NLP ðŸ¤¯Programming.\n"
     ]
    }
   ],
   "source": [
    "f1=open('mydata.txt',encoding='utf-8')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cd348b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " 'ðŸ˜Ž',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'ðŸ‘¹',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'ðŸ¤©',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'ðŸ¤¯',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweet tokenizer\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "\n",
    "twk=TweetTokenizer()\n",
    "\n",
    "twk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e253800",
   "metadata": {},
   "source": [
    "## Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "08d95075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenizer\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenizer, it. Is it? ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "66008002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mitu.co.in/dataset\n",
    "#Download: student3.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8b3113c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('student3.tsv')\n",
    "data =f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cb2b69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21]]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=[]\n",
    "def std_tokenizer(data):\n",
    "    return re.split(r\"[\\n]+\", data)\n",
    "tokens = std_tokenizer(data)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    list1.append(word_tokenize(token))\n",
    "    \n",
    "list1.pop(0)\n",
    "list1.pop(-1)\n",
    "\n",
    "list2 = [[int(x[0]), x[1], x[2], float(x[3]), int(x[4])] for x in list1]\n",
    "\n",
    "list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa3d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
