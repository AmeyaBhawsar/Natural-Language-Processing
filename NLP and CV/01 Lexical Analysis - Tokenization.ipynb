{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e6ec40",
   "metadata": {},
   "source": [
    "## Installations\n",
    "-pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8950cda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f26572",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: spacy in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (3.7.4)\n",
      "Requirement already satisfied: textblob in c:\\programdata\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (0.9.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (3.1.3)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in c:\\users\\administrator.dai-pc2\\appdata\\roaming\\python\\python311\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk spacy textblob -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ec7b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package indian to C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')  #tokenization\n",
    "nltk.download('stopwords') #stopwiords removal\n",
    "nltk.download('averaged_perceptron_tagger') #POS tagging\n",
    "nltk.download('wordnet') #wordnet database and lemmatization\n",
    "nltk.download('omw-1.4') #Stemming\n",
    "nltk.download('indian') #Indain language POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d0b3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Administrator.DAI-\n",
      "[nltk_data]     PC2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker') #chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46e23fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Sample example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb1bb82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='They told that their ages are 25 27 and 31 respectively '\n",
    "\n",
    "## find the average of ages mentioned in the above sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3b2c471",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "471d3ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'told',\n",
       " 'that',\n",
       " 'their',\n",
       " 'ages',\n",
       " 'are',\n",
       " '25',\n",
       " '27',\n",
       " 'and',\n",
       " '31',\n",
       " 'respectively']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c09041cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum=0\n",
    "cnt=0\n",
    "for i in s1:\n",
    "    if i.isnumeric():\n",
    "        cnt+=1\n",
    "        sum+=int(i)\n",
    "avg=sum/cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba87894b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.666666666666668\n"
     ]
    }
   ],
   "source": [
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c384cf3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85bd082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = []\n",
    "for word in sent.split():\n",
    "    if word.isdigit():\n",
    "        ages.append(int(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7205888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 27, 31]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12e7dfd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(ages) / len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fedc203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent='They told that their ages are 25 27 and 31 respectively '\n",
    "\n",
    "ages = [int(word) for word in sent.split() if word.isdigit()]\n",
    "np.sum(ages) / len(ages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aeb36a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27.666666666666668"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean([int(word) for word in sent.split() if word.isdigit()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437422b",
   "metadata": {},
   "source": [
    "### tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8212e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='Hello friends! How are you? Welcome to Python Programming.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8b2c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the functions\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "edb925d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello friends!', 'How are you?', 'Welcome to Python Programming.']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#segmenation:convert entire sentence into chunks of sentences\n",
    "sent_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12f72347",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'friends',\n",
       " '!',\n",
       " 'How',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'Python',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert entire sentence into separate words\n",
    "wrd=word_tokenize(sent)\n",
    "wrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3c3d421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "puc=['!','?','.']\n",
    "for i in wrd:\n",
    "    if i in puc:\n",
    "        cnt+=1\n",
    "percentage=(cnt/len(wrd))*100\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f66a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in wrd:\n",
    "    if not i.isalnum():\n",
    "        cnt+=1\n",
    "percentage=(cnt/len(wrd))*100\n",
    "print(percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18e2d2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Percentage is :  0.25\n"
     ]
    }
   ],
   "source": [
    "text = len([word for word in word_tokenize(sent) if not word.isalnum()])\n",
    "print(text)\n",
    "print(\"Percentage is : \",text / len(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01e5a5e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('y')   #asci value of any characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c102ea61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('K')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7c34f905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function getsizeof in module sys:\n",
      "\n",
      "getsizeof(...)\n",
      "    getsizeof(object [, default]) -> int\n",
      "    \n",
      "    Return the size of object in bytes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sys.getsizeof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3b28eeb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof('K')\n",
    "sys.getsizeof('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "345723f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = 'abcdef'\n",
    "sys.getsizeof(char) # 50 initial, afterward 1 char 1 byte -> 50+1+1+1+1+1 = 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7f8fb7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97) #ascii value to character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "56891105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ಅ'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(3205)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "341d3472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "व\n"
     ]
    }
   ],
   "source": [
    "char='\\u0935'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dc009044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "वी\n"
     ]
    }
   ],
   "source": [
    "char='\\u0935\\u0940'\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cf9092bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'鍐'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0x9350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bec3cd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof('鍐')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6e1c2126",
   "metadata": {},
   "outputs": [],
   "source": [
    "name='अमेय के प्रणव'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a0b4663f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['अमेय', 'के', 'प्रणव']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ca7c9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_text='यह पुणे के दक्षिण-पश्चिम में लगभग 60 किमी (37 मील) और सह्याद्रि में नासरपुर से लगभग 15 कि.मी. (9.3 मील) दूर स्थित है। यह किला समुद्र तल से 1,376 मीटर (4,514 फीट) ऊँचा है। किले के आधार का व्यास लगभग 40 किमी (25 मील) है, जिसने इस पर इतिहास में घेराबंदी करना मुश्किल बना दिया था। किले के खंडहरों में पानी के कुंड और गुफाएँ पाएँ गए हैं। यह किला मुरुमादेवी डोंगर (देवी मुरुम्बा का पर्वत) नामक पहाड़ी पर बनाया गया था। माना जाता है कि इसकी सुरक्षित भौगोलिक स्थिति के कारण ही छत्रपती शिवाजी महाराज ने इसे कई साल शासन का केन्द्र बनाएं रखा।[2][3]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "71efe836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['यह',\n",
       " 'पुणे',\n",
       " 'के',\n",
       " 'दक्षिण-पश्चिम',\n",
       " 'में',\n",
       " 'लगभग',\n",
       " '60',\n",
       " 'किमी',\n",
       " '(',\n",
       " '37',\n",
       " 'मील',\n",
       " ')',\n",
       " 'और',\n",
       " 'सह्याद्रि',\n",
       " 'में',\n",
       " 'नासरपुर',\n",
       " 'से',\n",
       " 'लगभग',\n",
       " '15',\n",
       " 'कि.मी',\n",
       " '.',\n",
       " '(',\n",
       " '9.3',\n",
       " 'मील',\n",
       " ')',\n",
       " 'दूर',\n",
       " 'स्थित',\n",
       " 'है।',\n",
       " 'यह',\n",
       " 'किला',\n",
       " 'समुद्र',\n",
       " 'तल',\n",
       " 'से',\n",
       " '1,376',\n",
       " 'मीटर',\n",
       " '(',\n",
       " '4,514',\n",
       " 'फीट',\n",
       " ')',\n",
       " 'ऊँचा',\n",
       " 'है।',\n",
       " 'किले',\n",
       " 'के',\n",
       " 'आधार',\n",
       " 'का',\n",
       " 'व्यास',\n",
       " 'लगभग',\n",
       " '40',\n",
       " 'किमी',\n",
       " '(',\n",
       " '25',\n",
       " 'मील',\n",
       " ')',\n",
       " 'है',\n",
       " ',',\n",
       " 'जिसने',\n",
       " 'इस',\n",
       " 'पर',\n",
       " 'इतिहास',\n",
       " 'में',\n",
       " 'घेराबंदी',\n",
       " 'करना',\n",
       " 'मुश्किल',\n",
       " 'बना',\n",
       " 'दिया',\n",
       " 'था।',\n",
       " 'किले',\n",
       " 'के',\n",
       " 'खंडहरों',\n",
       " 'में',\n",
       " 'पानी',\n",
       " 'के',\n",
       " 'कुंड',\n",
       " 'और',\n",
       " 'गुफाएँ',\n",
       " 'पाएँ',\n",
       " 'गए',\n",
       " 'हैं।',\n",
       " 'यह',\n",
       " 'किला',\n",
       " 'मुरुमादेवी',\n",
       " 'डोंगर',\n",
       " '(',\n",
       " 'देवी',\n",
       " 'मुरुम्बा',\n",
       " 'का',\n",
       " 'पर्वत',\n",
       " ')',\n",
       " 'नामक',\n",
       " 'पहाड़ी',\n",
       " 'पर',\n",
       " 'बनाया',\n",
       " 'गया',\n",
       " 'था।',\n",
       " 'माना',\n",
       " 'जाता',\n",
       " 'है',\n",
       " 'कि',\n",
       " 'इसकी',\n",
       " 'सुरक्षित',\n",
       " 'भौगोलिक',\n",
       " 'स्थिति',\n",
       " 'के',\n",
       " 'कारण',\n",
       " 'ही',\n",
       " 'छत्रपती',\n",
       " 'शिवाजी',\n",
       " 'महाराज',\n",
       " 'ने',\n",
       " 'इसे',\n",
       " 'कई',\n",
       " 'साल',\n",
       " 'शासन',\n",
       " 'का',\n",
       " 'केन्द्र',\n",
       " 'बनाएं',\n",
       " 'रखा।',\n",
       " '[',\n",
       " '2',\n",
       " ']',\n",
       " '[',\n",
       " '3',\n",
       " ']']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(h_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d575f5",
   "metadata": {},
   "source": [
    "### Space tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "18427193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! how are you?\n",
      "\n",
      "Welcome to the world of NLP.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "f1=open('mydata.txt','r')\n",
    "for ln in f1:\n",
    "    print(ln)\n",
    "f1.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f3e7d9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "411e95f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " '\\thow',\n",
       " 'are',\n",
       " 'you?\\nWelcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " '\\tProgramming.']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#space tokenizer\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "tk=SpaceTokenizer()\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d211cdf",
   "metadata": {},
   "source": [
    "## Tab Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "143b3436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9bdb0834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! ',\n",
       " 'how are you?\\nWelcome to the world of NLP ',\n",
       " 'Programming.\\n']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tab tokenizer\n",
    "from nltk.tokenize import TabTokenizer\n",
    "\n",
    "tk=TabTokenizer()\n",
    "\n",
    "tk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573f1dfd",
   "metadata": {},
   "source": [
    "## Line tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cffe9cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "761ef27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Friends! \\thow are you?', 'Welcome to the world of NLP \\tProgramming.']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#line tokenizer\n",
    "from nltk.tokenize import LineTokenizer\n",
    "\n",
    "lk=LineTokenizer()\n",
    "\n",
    "lk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01d6e5",
   "metadata": {},
   "source": [
    "### White space tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "495cca31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e0aee4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends!',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " 'Programming.']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#White tokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "wk=WhitespaceTokenizer()\n",
    "\n",
    "wk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc5943d",
   "metadata": {},
   "source": [
    "### MWE tokenizer (Multi word expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ea66e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends! \thow are you?\n",
      "Welcome to the world of NLP \tProgramming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1=open('mydata.txt','r')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "10d757e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " 'o',\n",
       " ' ',\n",
       " 'F',\n",
       " 'r',\n",
       " 'i',\n",
       " 'e',\n",
       " 'n',\n",
       " 'd',\n",
       " 's',\n",
       " '!',\n",
       " ' ',\n",
       " '\\t',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'a',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 'y',\n",
       " 'o',\n",
       " 'u',\n",
       " '?',\n",
       " '\\n',\n",
       " 'W',\n",
       " 'e',\n",
       " 'l',\n",
       " 'c',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'l',\n",
       " 'd',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'N',\n",
       " 'L',\n",
       " 'P',\n",
       " ' ',\n",
       " '\\t',\n",
       " 'P',\n",
       " 'r',\n",
       " 'o',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'm',\n",
       " 'm',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " '.',\n",
       " '\\n']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#White tokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "mwk=MWETokenizer()\n",
    "\n",
    "mwk.tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c9c346fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1 = 'Guido Van Rossum (Dutch: born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f9042a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Guido',\n",
       " 'Van Rossum',\n",
       " '(',\n",
       " 'Dutch',\n",
       " ':',\n",
       " 'born',\n",
       " '31',\n",
       " 'January',\n",
       " '1956',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'Dutch',\n",
       " 'programmer',\n",
       " 'best',\n",
       " 'known',\n",
       " 'as',\n",
       " 'the',\n",
       " 'creator',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Python',\n",
       " 'programming',\n",
       " 'language']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MWE tokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "\n",
    "mwk=MWETokenizer(separator=' ')  #separator given as space\n",
    "\n",
    "mwk.add_mwe(('Van', 'Rossum'))  #You want some words together always\n",
    "\n",
    "mwk.tokenize(word_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270ac4b",
   "metadata": {},
   "source": [
    "## Tweet tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1e447b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello Frie:)nds :)!  how are you?:( Welcome to the world of NLP  :D Programming.'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent =  'Hello Frie:)nds :)!  how are you?:( Welcome to the world of NLP  :D Programming.'\n",
    "\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "9e1fd815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Frie',\n",
       " ':)',\n",
       " 'nds',\n",
       " ':)',\n",
       " '!',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " ':(',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " ':D',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweet tokenizer\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "\n",
    "twk=TweetTokenizer()\n",
    "\n",
    "twk.tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "68410165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!  😎how are you?👹?\n",
      "Welcome to the 🤩 world of NLP 🤯Programming.\n"
     ]
    }
   ],
   "source": [
    "f1=open('mydata.txt',encoding='utf-8')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "9c97afb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " '😎how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " '👹',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " '🤩',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " '🤯Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a9d6fe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Friends!  😎how are you?👹?\n",
      "Welcome to the 🤩 world of NLP 🤯Programming.\n"
     ]
    }
   ],
   "source": [
    "f1=open('mydata.txt',encoding='utf-8')\n",
    "data=f1.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "cd348b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Friends',\n",
       " '!',\n",
       " '😎',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " '?',\n",
       " '👹',\n",
       " '?',\n",
       " 'Welcome',\n",
       " 'to',\n",
       " 'the',\n",
       " '🤩',\n",
       " 'world',\n",
       " 'of',\n",
       " 'NLP',\n",
       " '🤯',\n",
       " 'Programming',\n",
       " '.']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tweet tokenizer\n",
    "from nltk.tokenize import TweetTokenizer  \n",
    "\n",
    "twk=TweetTokenizer()\n",
    "\n",
    "twk.tokenize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e253800",
   "metadata": {},
   "source": [
    "## Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "08d95075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "This\n",
      "is\n",
      "some\n",
      "text\n",
      "with\n",
      "punctuation\n",
      ">\n",
      "Let's\n",
      "tokenizer\n",
      "it\n",
      "Is\n",
      "it\n",
      "ok\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def custom_tokenizer(text):\n",
    "    return re.split(r\"[.,;?!\\s]+\", text)\n",
    "\n",
    "text = \"This is some text with punctuation > Let's tokenizer, it. Is it? ok?\"\n",
    "\n",
    "tokens = custom_tokenizer(text)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "66008002",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mitu.co.in/dataset\n",
    "#Download: student3.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8b3113c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roll\tname\tclass\tmarks\tage\n",
      "1\tanil\tTE\t56.77\t22\n",
      "2\tamit\tTE\t59.77\t21\n",
      "3\taniket\tBE\t76.88\t19\n",
      "4\tajinkya\tTE\t69.66\t20\n",
      "5\tasha\tTE\t63.28\t20\n",
      "6\tayesha\tBE\t49.55\t20\n",
      "7\tamar\tBE\t65.34\t19\n",
      "8\tamita\tBE\t68.33\t23\n",
      "9\tamol\tTE\t56.75\t20\n",
      "10\tanmol\tBE\t78.66\t21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('student3.tsv')\n",
    "data =f.read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "cb2b69fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 'anil', 'TE', 56.77, 22],\n",
       " [2, 'amit', 'TE', 59.77, 21],\n",
       " [3, 'aniket', 'BE', 76.88, 19],\n",
       " [4, 'ajinkya', 'TE', 69.66, 20],\n",
       " [5, 'asha', 'TE', 63.28, 20],\n",
       " [6, 'ayesha', 'BE', 49.55, 20],\n",
       " [7, 'amar', 'BE', 65.34, 19],\n",
       " [8, 'amita', 'BE', 68.33, 23],\n",
       " [9, 'amol', 'TE', 56.75, 20],\n",
       " [10, 'anmol', 'BE', 78.66, 21]]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1=[]\n",
    "def std_tokenizer(data):\n",
    "    return re.split(r\"[\\n]+\", data)\n",
    "tokens = std_tokenizer(data)\n",
    "\n",
    "print(\"Tokens:\")\n",
    "for token in tokens:\n",
    "    list1.append(word_tokenize(token))\n",
    "    \n",
    "list1.pop(0)\n",
    "list1.pop(-1)\n",
    "\n",
    "list2 = [[int(x[0]), x[1], x[2], float(x[3]), int(x[4])] for x in list1]\n",
    "\n",
    "list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfa3d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
