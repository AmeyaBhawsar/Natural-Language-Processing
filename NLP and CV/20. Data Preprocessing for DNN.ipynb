{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9183715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca0fa6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10333902",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ['It was a nice rainy day.', 'The things are so beautiful in his point.',\n",
    "        'When your focus is clear , you won.','Many many happy returns of the day.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "367ca110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was a nice rainy day.',\n",
       " 'The things are so beautiful in his point.',\n",
       " 'When your focus is clear , you won.',\n",
       " 'Many many happy returns of the day.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f19599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dab266d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'nice': 1,\n",
       "             'it': 1,\n",
       "             'rainy': 1,\n",
       "             'a': 1,\n",
       "             'day': 2,\n",
       "             'was': 1,\n",
       "             'point': 1,\n",
       "             'his': 1,\n",
       "             'so': 1,\n",
       "             'the': 2,\n",
       "             'things': 1,\n",
       "             'in': 1,\n",
       "             'are': 1,\n",
       "             'beautiful': 1,\n",
       "             'focus': 1,\n",
       "             'you': 1,\n",
       "             'your': 1,\n",
       "             'when': 1,\n",
       "             'is': 1,\n",
       "             'won': 1,\n",
       "             'clear': 1,\n",
       "             'returns': 1,\n",
       "             'many': 1,\n",
       "             'happy': 1,\n",
       "             'of': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccb7db62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'day': 1,\n",
       " 'the': 2,\n",
       " 'many': 3,\n",
       " 'it': 4,\n",
       " 'was': 5,\n",
       " 'a': 6,\n",
       " 'nice': 7,\n",
       " 'rainy': 8,\n",
       " 'things': 9,\n",
       " 'are': 10,\n",
       " 'so': 11,\n",
       " 'beautiful': 12,\n",
       " 'in': 13,\n",
       " 'his': 14,\n",
       " 'point': 15,\n",
       " 'when': 16,\n",
       " 'your': 17,\n",
       " 'focus': 18,\n",
       " 'is': 19,\n",
       " 'clear': 20,\n",
       " 'you': 21,\n",
       " 'won': 22,\n",
       " 'happy': 23,\n",
       " 'returns': 24,\n",
       " 'of': 25}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "798a905a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = tokenizer.texts_to_matrix(lines)\n",
    "mat             # tokenizer.word_index from this it has 25 words and where its 1 then that \n",
    "                # word is prest or if 0 word not present in that sentence we have 4 sentencs in \n",
    "                # lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b31d2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d874c201",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq=tokenizer.texts_to_sequences(lines) # another method directly use numbers assign to that\n",
    "                                       # particular word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a53a2ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 5, 6, 7, 8, 1],\n",
       " [2, 9, 10, 11, 12, 13, 14, 15],\n",
       " [16, 17, 18, 19, 20, 21, 22],\n",
       " [3, 3, 23, 24, 25, 2, 1]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a5940dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6,  7,  8,  1,  0,  0,  0,  0],\n",
       "       [ 2,  9, 10, 11, 12, 13, 14, 15,  0,  0],\n",
       "       [16, 17, 18, 19, 20, 21, 22,  0,  0,  0],\n",
       "       [ 3,  3, 23, 24, 25,  2,  1,  0,  0,  0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded=pad_sequences(seq,maxlen=10,padding='post')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "075dd952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  4,  5,  6,  7,  8,  1],\n",
       "       [ 0,  0,  2,  9, 10, 11, 12, 13, 14, 15],\n",
       "       [ 0,  0,  0, 16, 17, 18, 19, 20, 21, 22],\n",
       "       [ 0,  0,  0,  3,  3, 23, 24, 25,  2,  1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can use post only bcz as neural network start learning first from left sides\n",
    "padded=pad_sequences(seq,maxlen=10,padding='pre')\n",
    "padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f62f0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976b5e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
